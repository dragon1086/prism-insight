#!/usr/bin/env python3
"""
YouTube Event Fund Crawler - 'Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå' Analysis System

This script monitors the YouTube channel 'Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå' for new videos,
transcribes them using OpenAI Whisper API, analyzes the content, and provides
contrarian investment recommendations.

Workflow:
1. Fetch latest videos from RSS feed
2. Compare with previous video list (stored in JSON)
3. Extract audio and transcribe with Whisper API
4. Analyze content and generate contrarian investment recommendations
5. Log results (future: integrate with automated trading)
"""

import os
import sys
import json
import logging
import asyncio
import yaml
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Third-party imports
import feedparser
import yt_dlp
from openai import OpenAI
from mcp_agent.agents.agent import Agent
from mcp_agent.app import MCPApp
from mcp_agent.workflows.llm.augmented_llm import RequestParams
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

# Setup directories
EVENTS_DIR = Path("events")
EVENTS_DIR.mkdir(exist_ok=True)

# Configure logging
log_file = EVENTS_DIR / f"youtube_crawler_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file)
    ]
)
logger = logging.getLogger(__name__)

# Constants
CHANNEL_ID = "UCznImSIaxZR7fdLCICLdgaQ"  # Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå
RSS_URL = f"https://www.youtube.com/feeds/videos.xml?channel_id={CHANNEL_ID}"
VIDEO_HISTORY_FILE = EVENTS_DIR / "youtube_video_history.json"
AUDIO_FILE = EVENTS_DIR / "temp_audio.mp3"


class YouTubeEventFundCrawler:
    """Main crawler class for YouTube event fund analysis"""

    def __init__(self):
        """Initialize crawler with OpenAI client"""
        # Load API key from mcp_agent.secrets.yaml
        secrets_file = Path("mcp_agent.secrets.yaml")
        if not secrets_file.exists():
            raise FileNotFoundError(
                "mcp_agent.secrets.yaml not found. "
                "Please copy mcp_agent.secrets.yaml.example and configure your API keys."
            )

        with open(secrets_file, 'r', encoding='utf-8') as f:
            secrets = yaml.safe_load(f)

        openai_api_key = secrets.get('openai', {}).get('api_key')
        if not openai_api_key or openai_api_key == "example key":
            raise ValueError(
                "OPENAI_API_KEY not found or not configured in mcp_agent.secrets.yaml. "
                "Please set openai.api_key in the secrets file."
            )

        self.openai_client = OpenAI(api_key=openai_api_key)
        logger.info("OpenAI client initialized successfully")

    def fetch_latest_videos(self) -> List[Dict[str, str]]:
        """
        Fetch latest videos from RSS feed

        Returns:
            List of video dictionaries with id, title, published, link
        """
        logger.info(f"Fetching RSS feed from: {RSS_URL}")

        try:
            feed = feedparser.parse(RSS_URL)
            videos = []

            for entry in feed.entries:
                video = {
                    'id': entry.yt_videoid,
                    'title': entry.title,
                    'published': entry.published,
                    'link': entry.link,
                    'author': entry.author if hasattr(entry, 'author') else 'Unknown'
                }
                videos.append(video)

            logger.info(f"Found {len(videos)} videos in feed")
            return videos

        except Exception as e:
            logger.error(f"Error fetching RSS feed: {e}", exc_info=True)
            return []

    def load_previous_videos(self) -> List[Dict[str, str]]:
        """
        Load previous video list from JSON file

        Returns:
            List of previous video dictionaries
        """
        if not Path(VIDEO_HISTORY_FILE).exists():
            logger.info("No previous video history found")
            return []

        try:
            with open(VIDEO_HISTORY_FILE, 'r', encoding='utf-8') as f:
                videos = json.load(f)
            logger.info(f"Loaded {len(videos)} previous videos")
            return videos
        except Exception as e:
            logger.error(f"Error loading video history: {e}", exc_info=True)
            return []

    def save_video_history(self, videos: List[Dict[str, str]]):
        """
        Save current video list to JSON file

        Args:
            videos: List of video dictionaries
        """
        try:
            with open(VIDEO_HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(videos, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved {len(videos)} videos to history")
        except Exception as e:
            logger.error(f"Error saving video history: {e}", exc_info=True)

    def find_new_videos(self, current_videos: List[Dict], previous_videos: List[Dict]) -> List[Dict]:
        """
        Find new videos by comparing current and previous lists

        Args:
            current_videos: Current video list
            previous_videos: Previous video list

        Returns:
            List of new video dictionaries
        """
        previous_ids = {video['id'] for video in previous_videos}
        new_videos = [video for video in current_videos if video['id'] not in previous_ids]

        logger.info(f"Found {len(new_videos)} new videos")
        return new_videos

    def extract_audio(self, video_url: str) -> Optional[str]:
        """
        Extract audio from YouTube video using yt-dlp

        Args:
            video_url: YouTube video URL

        Returns:
            Path to extracted audio file, or None on failure
        """
        logger.info(f"Extracting audio from: {video_url}")

        # Remove existing audio file if present
        if AUDIO_FILE.exists():
            AUDIO_FILE.unlink()

        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': str(EVENTS_DIR / 'temp_audio.%(ext)s'),
            'postprocessors': [{
                'key': 'FFmpegExtractAudio',
                'preferredcodec': 'mp3',
            }],
            'quiet': True,
            'no_warnings': True,
        }

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([video_url])

            if AUDIO_FILE.exists():
                logger.info("Audio extraction successful")
                return str(AUDIO_FILE)
            else:
                logger.error("Audio file not found after extraction")
                return None

        except Exception as e:
            logger.error(f"Error extracting audio: {e}", exc_info=True)
            return None

    def transcribe_audio(self, audio_file: str) -> Optional[str]:
        """
        Transcribe audio using OpenAI Whisper API

        Args:
            audio_file: Path to audio file

        Returns:
            Transcribed text, or None on failure
        """
        logger.info(f"Transcribing audio file: {audio_file}")

        try:
            with open(audio_file, "rb") as f:
                result = self.openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    language="ko"
                )

            transcript = result.text
            logger.info(f"Transcription successful ({len(transcript)} characters)")
            return transcript

        except Exception as e:
            logger.error(f"Error transcribing audio: {e}", exc_info=True)
            return None

    def create_analysis_agent(self, video_info: Dict, transcript: str) -> Agent:
        """
        Create AI agent for content analysis and investment recommendation

        Args:
            video_info: Video metadata dictionary
            transcript: Transcribed text

        Returns:
            Configured Agent instance
        """
        instruction = f"""ÎãπÏã†ÏùÄ Ïú†ÌäúÎ∏å Ï±ÑÎÑê 'Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå'Ïùò ÏΩòÌÖêÏ∏†Î•º Î∂ÑÏÑùÌïòÎäî Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.

## Î∂ÑÏÑù ÎåÄÏÉÅ ÏòÅÏÉÅ
- Ï†úÎ™©: {video_info['title']}
- Í≤åÏãúÏùº: {video_info['published']}
- URL: {video_info['link']}

## ÏòÅÏÉÅ ÏûêÎßâ Ï†ÑÎ¨∏
{transcript}

## Î∂ÑÏÑù Í≥ºÏ†ú

### 1Îã®Í≥Ñ: ÏΩòÌÖêÏ∏† Ïú†Ìòï ÌåêÎ≥Ñ
Îã§ÏùåÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî:
- Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏Ïù¥ ÏßÅÏ†ë Ï∂úÏó∞ÌïòÏó¨ ÏùòÍ≤¨ÏùÑ Ï†úÏãúÌïòÎäî ÏòÅÏÉÅÏù∏Í∞Ä?
- Îã®Ïàú Îâ¥Ïä§ ÏöîÏïΩÏù¥ÎÇò Í≤åÏä§Ìä∏ Ïù∏ÌÑ∞Î∑∞Îßå ÏûàÎäî ÏòÅÏÉÅÏùÄ ÏïÑÎãåÍ∞Ä?

**ÌåêÎ≥Ñ Í≤∞Í≥º**: "Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏ ÏùòÍ≤¨" ÎòêÎäî "Ïä§ÌÇµ ÎåÄÏÉÅ" Ï§ë ÌïòÎÇòÎ°ú Î™ÖÏãú

### 2Îã®Í≥Ñ: ÏãúÏû• Ï†ÑÎßù Î∂ÑÏÑù (Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏ ÏùòÍ≤¨Ïù∏ Í≤ΩÏö∞Îßå)
Ï†ÑÏù∏Íµ¨Í∞Ä ÏãúÏû•Ïóê ÎåÄÌï¥ Ïñ¥Îñ§ Í∏∞Ï°∞Î°ú ÎßêÌïòÍ≥† ÏûàÎäîÏßÄ Î∂ÑÏÑù:
- **ÏÉÅÏäπ Í∏∞Ï°∞**: ÎÇôÍ¥ÄÏ†Å Ï†ÑÎßù, Îß§Ïàò Ï∂îÏ≤ú, Í∏çÏ†ïÏ†Å ÏãúÍ∑∏ÎÑê Í∞ïÏ°∞
- **ÌïòÎùΩ Í∏∞Ï°∞**: ÎπÑÍ¥ÄÏ†Å Ï†ÑÎßù, Îß§ÎèÑ/Í¥ÄÎßù Ï∂îÏ≤ú, Î∂ÄÏ†ïÏ†Å ÏãúÍ∑∏ÎÑê Í∞ïÏ°∞
- **Ï§ëÎ¶Ω Í∏∞Ï°∞**: Î™ÖÌôïÌïú Î∞©Ìñ•ÏÑ± ÏóÜÏùå

**ÏãúÏû• Í∏∞Ï°∞ ÌåêÎã®**: ÏÉÅÏäπ/ÌïòÎùΩ/Ï§ëÎ¶Ω Ï§ë ÌïòÎÇòÎ°ú Î™ÖÏãú
**Í∑ºÍ±∞**: ÏûêÎßâÏóêÏÑú Ìï¥Îãπ ÌåêÎã®ÏùÑ ÎÇ¥Î¶∞ ÌïµÏã¨ Î∞úÏñ∏ Ïù∏Ïö© (3-5Í∞ú)

### 3Îã®Í≥Ñ: ÏΩòÌÖêÏ∏† ÏöîÏïΩ
ÏòÅÏÉÅÏùò ÌïµÏã¨ ÎÇ¥Ïö©ÏùÑ 3-5Í∞ú Î∂àÎ¶ø Ìè¨Ïù∏Ìä∏Î°ú ÏöîÏïΩ
- Ï£ºÏöî ÎÖºÏ†ê
- Ïñ∏Í∏âÎêú Í≤ΩÏ†ú ÏßÄÌëúÎÇò Ïù¥Ïäà
- Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïñ∏Í∏âÎêú Ï¢ÖÎ™©/ÏÑπÌÑ∞ (ÏûàÎäî Í≤ΩÏö∞)

### 4Îã®Í≥Ñ: Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎûµ (Contrarian Investment)
Ï†ÑÏù∏Íµ¨Ïùò ÏùòÍ≤¨Í≥º **Î∞òÎåÄ** Î∞©Ìñ•ÏúºÎ°ú Î≤†ÌåÖÌïòÎäî Ï†ÑÎûµ Ï†úÏãú:

**ÎßåÏïΩ ÏÉÅÏäπ Í∏∞Ï°∞ÎùºÎ©¥ (ÌïòÎùΩÏóê Î≤†ÌåÖ)**:
- Ïù∏Î≤ÑÏä§(Inverse) ETF/ETN Ï∂îÏ≤ú
  - KODEX Ïù∏Î≤ÑÏä§ (114800)
  - TIGER Ïù∏Î≤ÑÏä§ (252670)
  - KODEX ÏΩîÏä§Îã•150 Ïù∏Î≤ÑÏä§ (251340)
- Î∞©Ïñ¥Ï£º Ï∂îÏ≤ú (Ìó¨Ïä§ÏºÄÏñ¥, ÌïÑÏàòÏÜåÎπÑÏû¨ Îì±)
- ÌíãÏòµÏÖò Ï†ÑÎûµ Í∞ÄÎä• Ï¢ÖÎ™©

**ÎßåÏïΩ ÌïòÎùΩ Í∏∞Ï°∞ÎùºÎ©¥ (ÏÉÅÏäπÏóê Î≤†ÌåÖ)**:
- Î†àÎ≤ÑÎ¶¨ÏßÄ(Leverage) ETF/ETN Ï∂îÏ≤ú
  - KODEX Î†àÎ≤ÑÎ¶¨ÏßÄ (122630)
  - TIGER Î†àÎ≤ÑÎ¶¨ÏßÄ (233740)
  - KODEX ÏΩîÏä§Îã•150 Î†àÎ≤ÑÎ¶¨ÏßÄ (233160)
- ÏÑ±Ïû•Ï£º/Î™®Î©òÌÖÄÏ£º Ï∂îÏ≤ú
- ÏΩúÏòµÏÖò Ï†ÑÎûµ Í∞ÄÎä• Ï¢ÖÎ™©

**ÎßåÏïΩ Ï§ëÎ¶Ω Í∏∞Ï°∞ÎùºÎ©¥**:
- Í¥ÄÎßù Ï∂îÏ≤ú
- Î≥ÄÎèôÏÑ± Í¥ÄÎ†® ÏÉÅÌíà Í≤ÄÌÜ†

### 5Îã®Í≥Ñ: Î¶¨Ïä§ÌÅ¨ Í≤ΩÍ≥†
Ïó≠Î∞úÏÉÅ Ï†ÑÎûµÏùò Î¶¨Ïä§ÌÅ¨ Î™ÖÏãú:
- Ï†ÑÏù∏Íµ¨Ïùò ÏùòÍ≤¨Ïù¥ ÎßûÏùÑ Í≤ΩÏö∞Ïùò ÏÜêÏã§ ÏãúÎÇòÎ¶¨Ïò§
- Í∂åÏû• ÏÜêÏ†àÎß§ ÎπÑÏú® (Ïòà: -5%, -10%)
- Ìè¨ÏßÄÏÖò ÏÇ¨Ïù¥Ïßï Í∂åÏû• (Ï†ÑÏ≤¥ ÏûêÏÇ∞Ïùò Î™á %Î°ú Ï†úÌïú)

## Ï∂úÎ†• ÌòïÏãù
Îã§Ïùå ÌòïÏãùÏúºÎ°ú Íµ¨Ï°∞ÌôîÎêú Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï∂úÎ†•ÌïòÏÑ∏Ïöî:

```
# Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Î∂ÑÏÑù

## üì∫ ÏòÅÏÉÅ Ï†ïÎ≥¥
- **Ï†úÎ™©**: {video_info['title']}
- **Í≤åÏãúÏùº**: {video_info['published']}
- **URL**: {video_info['link']}

## 1Ô∏è‚É£ ÏΩòÌÖêÏ∏† Ïú†Ìòï ÌåêÎ≥Ñ
[Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏ ÏùòÍ≤¨ / Ïä§ÌÇµ ÎåÄÏÉÅ]

## 2Ô∏è‚É£ ÏãúÏû• Í∏∞Ï°∞ Î∂ÑÏÑù
**ÌåêÎã®**: [ÏÉÅÏäπ/ÌïòÎùΩ/Ï§ëÎ¶Ω]

**Í∑ºÍ±∞**:
- [Ïù∏Ïö©1]
- [Ïù∏Ïö©2]
- [Ïù∏Ïö©3]

## 3Ô∏è‚É£ ÏòÅÏÉÅ ÎÇ¥Ïö© ÏöîÏïΩ
- ÌïµÏã¨ ÎÖºÏ†ê 1
- ÌïµÏã¨ ÎÖºÏ†ê 2
- ÌïµÏã¨ ÎÖºÏ†ê 3

## 4Ô∏è‚É£ Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎûµ
### Ï∂îÏ≤ú Ìè¨ÏßÄÏÖò: [Îß§Ïàò/Îß§ÎèÑ/Í¥ÄÎßù]

### Ï∂îÏ≤ú Ï¢ÖÎ™©/ÏÉÅÌíà
1. **[Ï¢ÖÎ™©Î™Ö] (Ï¢ÖÎ™©ÏΩîÎìú)**
   - Ïú†Ìòï: [ETF/ETN/Í∞úÎ≥ÑÏ£º]
   - Ïù¥Ïú†: ...

2. **[Ï¢ÖÎ™©Î™Ö] (Ï¢ÖÎ™©ÏΩîÎìú)**
   - Ïú†Ìòï: [ETF/ETN/Í∞úÎ≥ÑÏ£º]
   - Ïù¥Ïú†: ...

### ÏßÑÏûÖ Ï†ÑÎûµ
- ÌÉÄÏù¥Î∞ç: ...
- Î∂ÑÌï†Îß§Ïàò Í∂åÏû•: ...

## 5Ô∏è‚É£ Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨
- ‚ö†Ô∏è ÏÜêÏ†àÎß§: -X% ÎèÑÎã¨ Ïãú Î¨¥Ï°∞Í±¥ Ï≤≠ÏÇ∞
- ‚ö†Ô∏è Ìè¨ÏßÄÏÖò ÌÅ¨Í∏∞: Ï†ÑÏ≤¥ ÏûêÏÇ∞Ïùò Y% Ïù¥ÌïòÎ°ú Ï†úÌïú
- ‚ö†Ô∏è Ï†ÑÏù∏Íµ¨ ÏùòÍ≤¨Ïù¥ ÎßûÏùÑ Í≤ΩÏö∞ ÏòàÏÉÅ ÏÜêÏã§: ...
```

## Ï£ºÏùòÏÇ¨Ìï≠
- ÏûêÎßâ ÎÇ¥Ïö©ÎßåÏùÑ Í∑ºÍ±∞Î°ú Î∂ÑÏÑùÌïòÏÑ∏Ïöî (Ï∂îÏ∏° Í∏àÏßÄ)
- Ï†ÑÏù∏Íµ¨Í∞Ä ÏßÅÏ†ë Ïñ∏Í∏âÌïòÏßÄ ÏïäÏùÄ Ï¢ÖÎ™©ÏùÄ Ïã†Ï§ëÌïòÍ≤å Ï∂îÏ≤úÌïòÏÑ∏Ïöî
- Ïó≠Î∞úÏÉÅ Ï†ÑÎûµÏùò ÎÜíÏùÄ Î¶¨Ïä§ÌÅ¨Î•º Î™ÖÌôïÌûà Í≤ΩÍ≥†ÌïòÏÑ∏Ïöî
- Ìà¨Ïûê Í∂åÏú†Í∞Ä ÏïÑÎãå Ï†ïÎ≥¥ Ï†úÍ≥µ Î™©Ï†ÅÏûÑÏùÑ Î™ÖÏãúÌïòÏÑ∏Ïöî
"""

        return Agent(
            name="youtube_event_fund_analyst",
            instruction=instruction,
            server_names=[]  # No MCP servers needed for transcript analysis
        )

    async def analyze_video(self, video_info: Dict, transcript: str) -> str:
        """
        Analyze video content using AI agent

        Args:
            video_info: Video metadata
            transcript: Transcribed text

        Returns:
            Analysis result text
        """
        logger.info(f"Analyzing video: {video_info['title']}")

        try:
            agent = self.create_analysis_agent(video_info, transcript)

            # Attach LLM to agent
            llm = await agent.attach_llm(OpenAIAugmentedLLM)

            # Generate analysis using the agent
            result = await llm.generate_str(
                message="ÏúÑ ÏßÄÏãúÏÇ¨Ìï≠Ïóê Îî∞Îùº ÏòÅÏÉÅÏùÑ Î∂ÑÏÑùÌïòÍ≥† Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎûµÏùÑ Ï†úÏãúÌï¥Ï£ºÏÑ∏Ïöî.",
                request_params=RequestParams(
                    model="gpt-4.1",
                    maxTokens=16000,
                    max_iterations=3,
                    parallel_tool_calls=False,
                    use_history=True
                )
            )

            logger.info("Analysis completed successfully")
            return result

        except Exception as e:
            logger.error(f"Error during analysis: {e}", exc_info=True)
            return f"Î∂ÑÏÑù Ïã§Ìå®: {str(e)}"

    def cleanup_temp_files(self):
        """Remove temporary audio files"""
        if AUDIO_FILE.exists():
            try:
                AUDIO_FILE.unlink()
                logger.info("Cleaned up temporary audio file")
            except Exception as e:
                logger.warning(f"Failed to clean up audio file: {e}")

    async def process_new_video(self, video_info: Dict) -> Optional[str]:
        """
        Process a new video: extract audio, transcribe, analyze

        Args:
            video_info: Video metadata dictionary

        Returns:
            Analysis result text, or None on failure
        """
        logger.info(f"Processing new video: {video_info['title']}")

        try:
            # Step 1: Extract audio
            audio_file = self.extract_audio(video_info['link'])
            if not audio_file:
                return None

            # Step 2: Transcribe audio
            transcript = self.transcribe_audio(audio_file)
            if not transcript:
                return None

            # Save transcript for debugging
            transcript_file = EVENTS_DIR / f"transcript_{video_info['id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(transcript_file, 'w', encoding='utf-8') as f:
                f.write(f"Video: {video_info['title']}\n")
                f.write(f"URL: {video_info['link']}\n")
                f.write(f"Published: {video_info['published']}\n")
                f.write(f"\n{'='*80}\n\n")
                f.write(transcript)
            logger.info(f"Transcript saved to: {transcript_file}")

            # Step 3: Analyze content
            analysis = await self.analyze_video(video_info, transcript)

            # Save analysis result
            analysis_file = EVENTS_DIR / f"analysis_{video_info['id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                f.write(analysis)
            logger.info(f"Analysis saved to: {analysis_file}")

            return analysis

        except Exception as e:
            logger.error(f"Error processing video: {e}", exc_info=True)
            return None

        finally:
            # Always cleanup temporary files
            self.cleanup_temp_files()

    async def process_single_video_url(self, video_url: str):
        """
        Process a single video URL directly (for testing)

        Args:
            video_url: YouTube video URL
        """
        logger.info("="*80)
        logger.info("YouTube Event Fund Crawler - Single Video Mode")
        logger.info("="*80)

        try:
            # Create video info from URL
            video_info = {
                'title': 'Manual Video Input',
                'published': datetime.now().isoformat(),
                'link': video_url,
                'id': video_url.split('=')[-1] if '=' in video_url else video_url.split('/')[-1]
            }

            logger.info(f"Processing video: {video_url}")

            analysis = await self.process_new_video(video_info)

            if analysis:
                # Print analysis to console
                print("\n" + "="*80)
                print("ANALYSIS RESULT")
                print("="*80)
                print(analysis)
                print("="*80 + "\n")
            else:
                logger.warning("Failed to analyze video")

            logger.info("="*80)
            logger.info("YouTube Event Fund Crawler - Completed")
            logger.info("="*80)

        except Exception as e:
            logger.error(f"Fatal error processing video: {e}", exc_info=True)
            raise

    async def run(self):
        """Main execution workflow"""
        logger.info("="*80)
        logger.info("YouTube Event Fund Crawler - Starting")
        logger.info("="*80)

        try:
            # Step 1: Fetch latest videos from RSS
            current_videos = self.fetch_latest_videos()
            if not current_videos:
                logger.warning("No videos found in RSS feed")
                return

            # Step 2: Load previous video history
            previous_videos = self.load_previous_videos()

            # Check if this is first run
            is_first_run = len(previous_videos) == 0

            if is_first_run:
                logger.info("üé¨ First run detected - initializing video history")
                logger.info(f"Found {len(current_videos)} videos in channel")
                logger.info("Saving video history without processing...")

                # Save current videos and exit
                self.save_video_history(current_videos)

                logger.info("="*80)
                logger.info("‚úÖ Video history initialized successfully")
                logger.info("üí° Run again to detect and process new videos")
                logger.info("="*80)
                return

            # Step 3: Find new videos
            new_videos = self.find_new_videos(current_videos, previous_videos)

            if not new_videos:
                logger.info("No new videos found")
                return

            # Step 4: Process each new video
            for video in new_videos:
                logger.info("\n" + "="*80)
                logger.info(f"Processing: {video['title']}")
                logger.info("="*80)

                analysis = await self.process_new_video(video)

                if analysis:
                    # Print analysis to console
                    print("\n" + "="*80)
                    print("ANALYSIS RESULT")
                    print("="*80)
                    print(analysis)
                    print("="*80 + "\n")
                else:
                    logger.warning(f"Failed to analyze video: {video['title']}")

            # Step 5: Save updated video history
            self.save_video_history(current_videos)

            logger.info("="*80)
            logger.info("YouTube Event Fund Crawler - Completed")
            logger.info("="*80)

        except Exception as e:
            logger.error(f"Fatal error in main workflow: {e}", exc_info=True)
            raise


async def main():
    """Entry point"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="YouTube Event Fund Crawler - Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Î∂ÑÏÑù",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Normal mode (monitor RSS feed for new videos)
  python youtube_event_fund_crawler.py

  # Test mode (process specific video URL)
  python youtube_event_fund_crawler.py --video-url "https://www.youtube.com/watch?v=VIDEO_ID"
        """
    )
    parser.add_argument(
        '--video-url',
        type=str,
        help='Process a specific YouTube video URL (test mode)'
    )

    args = parser.parse_args()

    try:
        crawler = YouTubeEventFundCrawler()

        if args.video_url:
            # Single video mode
            logger.info(f"üéØ Test mode: Processing single video")
            await crawler.process_single_video_url(args.video_url)
        else:
            # Normal RSS monitoring mode
            await crawler.run()

    except KeyboardInterrupt:
        logger.info("Interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
